---
title: "ANDREW_DELIS_fluent.io_case_revisited"
format:
  html:
    toc: true
    toc-depth: 3
    toc-location: left
    toc-title: "Contents"
execute:
  warning: false
  message: false
output: html_document
date: "2024-09-13"
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

# packages
library(rpact)
library(dplyr)
library(dbplyr)

# historical data on customer subscriptions and purchases in 2022:
hd <- read.csv( "https://raw.githubusercontent.com/jefftwebb/data/main/fluent_historical.csv" )

# hypothetical results for the A vs. B, A vs. C, and A vs. D trial
AB <- read.csv( "https://raw.githubusercontent.com/jefftwebb/data/main/AB_group_sequential_test.csv" )
AC <- read.csv( "https://raw.githubusercontent.com/jefftwebb/data/main/AC_group_sequential_test.csv" )
AD <- read.csv( "https://raw.githubusercontent.com/jefftwebb/data/main/AD_group_sequential_test.csv" )

```

### Estimated total sample size for the group sequential trials
```{r }

# use rpact library and given parameters to estimate the average sample size
design <- getDesignGroupSequential( typeOfDesign = "asOF", 
                                    informationRates = c( 0.2, 0.4, 0.6, 0.8, 1 ), 
                                    alpha = 0.05, 
                                    typeBetaSpending = "bsOF" )

summary( getSampleSizeMeans( design, 
                             alternative = 2,
                             stDev = 24 ) )

```

On average we will be able to stop the trial during the fourth sequence. The average sample size for a test like this is 2920.3, and the fourth trial begins at n = 2516 and ends at n = 3356 if the trial is not stopped earlier. This 2920.3 number is the n per group. The total expected n for the whole trial will would be 8760.

### What is the expected duration of the entire group sequential test?
```{r }

# new subscriptions
ns <- hd |>
  filter( type == "subscription" ) |>        # filter down to subscribers
  arrange( customer_id, date) |>             # arrange by customer and date
  slice( 1, .by = customer_id ) |>           # pick out each customer's first appearance in the data
  count( date ) |>                           # count the number of new rows (customers)
  summarize( mean( n ) |> ceiling() )        # calculate mean new subscribers and round up
  
ns$`ceiling(mean(n))`

2920.3 * 3 / ns$`ceiling(mean(n))`

```

### How does it compare to the A/B/C/D test duration in the previous case study?
The expected duration of the sequential test is shorter than the The A/B/C/D test of ARPU (122 days), and has a chance to be even shorter.

### Analysis of the hypothetical data for the A vs. B test
```{r }

#create output from design
# eb = efficacy boundary
# fb = futility boundary

n <- c( 840, 1678, 2518, 3356, 4196 )
eb <- c( 7.049, 3.389, 2.200, 1.626, 1.290 )
fb <- c( -2.376, -0.033, 0.6890, 1.034 )

# lk = look
comparison_function <- function( df, n, eb, fb, lk ) {
  
  # subset rows by look
  df <- df[ 1:n[ lk ], ]
  
  # caculate treatment effect as difference in means
  t <- ( mean( filter( df, group != "A" )$paid ) - mean( filter( df, group == "A" )$paid ) ) |> round( 1 )
  
  # format dates
  df$date <- as.Date( df$date )
  
  # calculate days as max(date) - min( date )
  days <- max( df$date ) - min( df$date )
  
  # format output
  ifelse( t > eb[lk] | t < fb[lk],
          paste( "stop test, t =", t, ", days =", days, ", subjects =", nrow( df ) ),
          paste( "continue test, t =", t ) )
  
}

# call the function for our AB data
comparison_function( AB, n = n, eb = eb, fb = fb, lk = 1 )
comparison_function( AB, n = n, eb = eb, fb = fb, lk = 2 )

```
For our test of A vs. B, we will be able to stop the experiment after the second sequence due to efficacy.

### Analysis of the hypothetical data for the A vs. C test
```{r }

comparison_function( AC, n = n, eb = eb, fb = fb, lk = 1 )
comparison_function( AC, n = n, eb = eb, fb = fb, lk = 2 )
comparison_function( AC, n = n, eb = eb, fb = fb, lk = 3 )

```
For the A vs. C test we will be able to stop the experiment after the third sequence due to efficacy

### Analysis of the hypothetical data for the A vs. D test
```{r }

comparison_function( AD, n = n, eb = eb, fb = fb, lk = 1 )

```
For the A vs. C test we will be able to stop the experiment after the first sequence due to futility

### Which method requires fewer customers and therefore fewer days to obtain a result?
```{r }

# n for the experiment
1678 + 2518 + 840 

# number of days
17 + 24 + 9

# difference in days
122 - 50

```

Because of group sequential testing, we will be able to cut the duration of the test down from 122 days to 50 days and still measure whether there is a difference in ARPU. This is still a long time for a test, and testing conversions will be much faster. However ARPU is probably more valuable to a company, and this will shave off 72 days of data gathering by introducing the ability to end the trial early.

### What is the effect size of the treatment for AC?
```{r }

set.seed(123)

# initialize the vector that will hold the simulated treatment effects
boot_dist <- 1:1000

# subset the hypothetical data
sub_ac <- AC[ 1:2518, ]

# run the simulation using a loop
for( i in 1:1000 ) {
  
  # sample df w/ replacement
  boot <- sample_frac( tbl = sub_ac, size = 1, replace = T)
  
  # calculate treatment effect and store in boot_dist
  boot_dist[ i ] <- ( mean( filter( boot, group == "C" )$paid ) - mean( filter( boot, group == "A" )$paid ) )
  
}

# the average of the bootstrap should resemble the observed effect
mean( boot_dist ) |> round( 1 )

# calculate upper and lower bounds
quantile( boot_dist, probs = c( 0.025, 0.975 ) ) |> round( 1 )

```
Based on these results we can be 95% confident that the real difference in means between A and C is 0.8 and 4.3.

### Pricing recommendation

Based on the results above, the B subscription is having the greatest influence on ARPU. The difference in the means during that portion of the sequential test was high enough that we were able to stop the test after the second sequence due to efficacy. For some reason this subscription is working really well. I would guess that the reason for this is that the perks in this subscription are worth the price jump. This means that you may even be able to raise the price of this package a little bit. 

At the same time I would merge the B and C subscriptions. The B subscription is doing well enough. We rejected the null hypothesis that these two groups produce the same amount of revenue. However the C subscription does not seem to be doing well. We failed to reject the null when running this test.

Merging the perks of the C subscription with the B subscription and keeping the B subscription at its current price of 7.99 while increasing the price of the D subscription would be my recommendation on pricing.
